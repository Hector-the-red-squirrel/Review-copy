{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "import allel\n",
    "import itertools\n",
    "import os\n",
    "from subprocess import call\n",
    "from tqdm import tqdm, trange\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "#allows multiple outputs: all, last, last_expr(default), none, last_expr_or_assign\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"last_expr_or_assign\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: make LD dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N/A for Bombus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Chop by 100k window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this step uses tabix\n",
    "#this first requires turning vcf to vcf.gz\n",
    "# bgzip your.vcf\n",
    "# tabix -p vcf your.vcf.gz <- this makes index\n",
    "# tabix your.vcf.gz chr1:10,000,000-20,000,000\n",
    "\n",
    "#bombus has to run in hap-r2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Calculating Mean-Median R2 per Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take all files and make one dataframe\n",
    "ld_r2 = []\n",
    "\n",
    "path_folder = '/data3/TaeFile/HapLd/'\n",
    "\n",
    "for file in tqdm(os.listdir(path_folder), total=len(path_folder)):\n",
    "    window = file.split('_headered')[0]\n",
    "    df = pd.read_csv(f'{path_folder}/{file}', sep='\\t')\n",
    "    mean_r2 = df['R^2'].mean()\n",
    "    median_r2 = df['R^2'].median()\n",
    "    ld_r2.append([window, mean_r2, median_r2])\n",
    "\n",
    "#destination of the final file is in home directory = windowed_LD.csv\n",
    "#condition, hap.ld, with maf 0.2, and window of 60 bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(ld_r2, columns = ['ID', 'r2_mean', 'r2_median'])\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Making the Master Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AA caller, based on frequency.\n",
    "#common, high frequency seen as reference/ancestral\n",
    "#rarer, low frequency seen as alternative/derived\n",
    "def AA_caller(frequency, reference, alternative):\n",
    "    if (frequency > 0.5):\n",
    "        return alternative\n",
    "    elif (frequency < 0.5):\n",
    "        return reference\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mutation direction function \"strength_classifier\"\n",
    "strong_bases= ['G', 'C']\n",
    "weak_bases= ['A', 'T']\n",
    "\n",
    "def strength_classifier(ancestor, derived):\n",
    "    if (ancestor in strong_bases) and (derived in weak_bases):\n",
    "        return 'SW'\n",
    "    elif (ancestor in weak_bases) and (derived in strong_bases):\n",
    "        return 'WS'\n",
    "    else:\n",
    "        return 'NN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define frequency of the Derived State, the mutation\n",
    "#if ALT = Derived, keep the original AF, which describes the frequency of the ALT\n",
    "#if REF = Derived, use 1 - AF\n",
    "\n",
    "def mutation_frequency (Derived, ALT, AF):\n",
    "    if Derived == ALT: #this means derived is ALT, which AF is associated with\n",
    "        return AF\n",
    "    if Derived != ALT: #this means dervied is REF, which is inversely associated with AF\n",
    "        return (1-AF)\n",
    "    else:\n",
    "        return 'Error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ancestry based on allele frequency\n",
    "#sorting based on mutation frequency\n",
    "\n",
    "def barcoder(strength, frequency):\n",
    "    if (strength == 'SW') and (frequency <= 0.1): #make it less or equal, to be generalizable for different data.\n",
    "        return 'SW-Rare'\n",
    "    elif (strength == 'SW') and (0.25 <= frequency <= 0.5):\n",
    "        return 'SW-Common'\n",
    "    elif (strength == 'WS') and (frequency <= 0.1):\n",
    "        return 'WS-Rare'\n",
    "    elif (strength == 'WS') and (0.25 <= frequency <= 0.5):\n",
    "        return 'WS-Common'\n",
    "    else:\n",
    "        return 'NaN' #keep it this way for Bombus, np.nan causes error downstream, I don't know why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#execute everything\n",
    "bombus_things = []\n",
    "\n",
    "path_folder_2 = '/data3/TaeFile/HeaderedVcf/'\n",
    "\n",
    "for file in tqdm(os.listdir(path_folder_2), total=len(path_folder_2)):\n",
    "    window = file.split('_headered')[0]\n",
    "    \n",
    "    # Process the file\n",
    "    df_basket = pd.read_table(f'{path_folder_2}/{file}', sep ='\\t', header=None, comment='#')\n",
    "    df_basket.rename(columns={\n",
    "        0:\"SCAF\", \n",
    "        1:\"POS\", \n",
    "        2:\"Id\", \n",
    "        3:\"REF\", \n",
    "        4:\"ALT\", \n",
    "        5:\"quality\", \n",
    "        6:\"filter\", \n",
    "        7:\"INFO\", \n",
    "        8:\"header\", \n",
    "        9:\"1\", 10:\"10\", 11:\"11\", 12:\"2b\", 13:\"3\", 14:\"4\", 15:\"5\", 16:\"6\", 17:\"7\", 18:\"8\"}, \n",
    "                     inplace=True)\n",
    "    column_picks= [\"SCAF\", \"POS\", \"REF\", \"ALT\", \"INFO\"]\n",
    "    df_basket_picks = df_basket[column_picks]\n",
    "    \n",
    "    # Get Allele Frequency\n",
    "    df_basket_picks['AF'] = df_basket_picks['INFO'].str.split('AF=').str.get(1).str.split(';').str.get(0).astype(float)\n",
    "    df_basket_picks.drop(columns=['INFO'], inplace=True)\n",
    "    \n",
    "    #Drop Allele Frequency of 0 and 1\n",
    "    df_basket_picks = df_basket_picks[df_basket_picks['AF'] != 1.0] #drop all AF of 1\n",
    "    df_basket_picks = df_basket_picks[df_basket_picks['AF'] != 0] #drop all AF of 0\n",
    "    \n",
    "    #AA base calling\n",
    "    df_basket_picks[\"AA\"] = df_basket_picks.apply(lambda row: AA_caller(row[\"AF\"], row[\"REF\"], row[\"ALT\"]), \n",
    "                                                  axis= 'columns')\n",
    "    df_basket_picks[\"Derived\"] = df_basket_picks.apply(lambda row: AA_caller(row[\"AF\"], row[\"ALT\"], row[\"REF\"]), \n",
    "                                                       axis= 'columns')\n",
    "    \n",
    "    #Mutation direction\n",
    "    df_basket_picks['Dirct'] = df_basket_picks.apply(lambda row: strength_classifier(row['REF'], row['ALT']), \n",
    "                                                         axis='columns')\n",
    "    \n",
    "    #Mutation Frequency, feed the variables in order of Derived, ALT, AF\n",
    "    df_basket_picks['MF'] = df_basket_picks.apply(lambda row: mutation_frequency(row['Derived'], row['ALT'], row['AF']), \n",
    "                                                  axis='columns')\n",
    "    \n",
    "    df_basket_picks['Barcode'] = df_basket_picks.apply(lambda row: barcoder(row['Dirct'], row['MF']), axis='columns')\n",
    "    #Barcoded_Bombus = df_basket_picks[df_basket_picks['Barcode'] != 'NaN'] #drop anything NaN <- this tosses NN\n",
    "        #this also got rid of any WS and SW that fell in 0.2 and 0.4 freq window. Now it keeps it all. = better\n",
    "    Barcoded_Bombus = df_basket_picks #maintain variable transition so that I don't have to touch anything downstream\n",
    "    \n",
    "    # dr.kent's stats\n",
    "    SW_Total_freq = (Barcoded_Bombus['Dirct'].values == 'SW').sum()\n",
    "    WS_Total_freq = (Barcoded_Bombus['Dirct'].values == 'WS').sum()\n",
    "    NN_Total_freq = (Barcoded_Bombus['Dirct'].values == 'NN').sum()\n",
    "    SNP_Total = SW_Total_freq + WS_Total_freq + NN_Total_freq\n",
    "    \n",
    "    SW_Rare_freq = (Barcoded_Bombus['Barcode'].values == 'SW-Rare').sum()\n",
    "    WS_Rare_freq = (Barcoded_Bombus['Barcode'].values == 'WS-Rare').sum()\n",
    "    \n",
    "    SW_Common_freq = (Barcoded_Bombus['Barcode'].values == 'SW-Common').sum()\n",
    "    WS_Common_freq = (Barcoded_Bombus['Barcode'].values == 'WS-Common').sum()\n",
    "    \n",
    "    bombus_things.append([window, SW_Total_freq, WS_Total_freq, NN_Total_freq, SNP_Total, SW_Rare_freq, WS_Rare_freq, \n",
    "                            SW_Common_freq, WS_Common_freq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_file = pd.DataFrame(bombus_things)\n",
    "final_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_file.rename(columns={\n",
    "        0:\"ID\", \n",
    "        1:\"SW_Total\", 2:\"WS_Total\", 3:\"NN_Total\", 4: \"SNP_Total\",\n",
    "        5:\"SW_Rare\", 6:\"WS_Rare\", 7:\"SW_Common\", 8:\"WS_Common\",}, inplace=True)\n",
    "\n",
    "#W_Total_freq, WS_Total_freq, NN_Total_freq, SNP_Total, SW_Rare_freq, WS_Rare_freq, SW_Common_freq, WS_Common_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_file.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start Merging R2 dataframe with the 'final file'\n",
    "merged_Bombus = df.merge(final_file, on='ID')\n",
    "Chopped_Bombus = merged_Bombus.dropna() #removes issue downstream\n",
    "Chopped_Bombus.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Chopped_Bombus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = final_file[final_file['ID'] == 'NT_177880.1_900000'] #checking if it paired up correctly\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run only once!\n",
    "Chopped_Bombus.to_csv('/home/taeyoon/VcfFiles/LdByWindow/BombusSFiles/Chopped_Bombus.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bombus_df = pd.read_csv('/home/taeyoon/VcfFiles/LdByWindow/BombusSFiles/Chopped_Bombus.csv')\n",
    "Bombus_df.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge with GC content per Window and Adjust Total Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bombus_GC = pd.read_csv('/home/taeyoon/GCContent/BimpGC_ready.csv', sep='\\t')\n",
    "Bombus_GC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge!\n",
    "Bombus_GC_df = pd.merge(Bombus_df, Bombus_GC, how='left', on=['ID'])\n",
    "Bombus_GC_df.head()\n",
    "len(Bombus_GC_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjusted Total values\n",
    "Bombus_GC_df['SW_T_Adjusted'] = Bombus_GC_df['SW_Total']/Bombus_GC_df['GC_Content']\n",
    "Bombus_GC_df['WS_T_Adjusted'] = Bombus_GC_df['WS_Total']/(1 - Bombus_GC_df['GC_Content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lambda, which is SW/WS\n",
    "Bombus_GC_df['Lambda'] = Bombus_GC_df['SW_T_Adjusted'] / Bombus_GC_df['WS_T_Adjusted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bombus_GC_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Bombus_GC_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concise, easier viewing\n",
    "Bombus_view = Bombus_GC_df.drop(columns=[\n",
    "    'SW_Total','WS_Total','NN_Total','SNP_Total','SW_Rare','WS_Rare','SW_Common','WS_Common'])\n",
    "Bombus_view.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean of lambda\n",
    "Bombus_view['Lambda'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GC content unadjusted\n",
    "(Bombus_GC_df['SW_Total']/Bombus_GC_df['WS_Total']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S1: Looking at Total numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name the the final table to work with x, drop possible NaNs\n",
    "x = Bombus_GC_df.dropna()\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot WS and SW together\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.ylim(0,2100)\n",
    "plt.xlim(0,1)\n",
    "\n",
    "#WS is blue\n",
    "WST_adj = sns.regplot(x['r2_mean'], x['WS_T_Adjusted'], marker=\"+\", scatter_kws={'alpha':0.5}, label='WS') \n",
    "\n",
    "#SW is orange\n",
    "sns.regplot(x['r2_mean'], x['SW_T_Adjusted'], marker=\"+\", scatter_kws={'alpha':0.5}, label='SW') \n",
    "\n",
    "plt.ylabel('GC% adjusted Mutation Counts')\n",
    "plt.xlabel('R^2 Mean')\n",
    "plt.legend(loc='upper right', prop={'size': 20}, markerscale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.linregress(x['r2_mean'], x['WS_T_Adjusted'])#WS is blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.linregress(x['r2_mean'], x['SW_T_Adjusted']) #SW is orange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z-test for coefficients (slopes)\n",
    "def Z_score(slope1, std_error1, slope2, std_error2):\n",
    "    numerator = (slope1 - slope2)\n",
    "    denominator = pow((pow(std_error1,2) + pow(std_error2,2)), 1/2)\n",
    "    Z = (numerator) / (denominator)\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z_score, input order in slope1, std_error1, slope2, std_error2\n",
    "Z_score(-556.3163997919412, 22.419131687548198, -1479.556418050194, 61.61696031931891)\n",
    "#Z score: 14.08047801884966\n",
    "#p-value: 5.007e-45, reject null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unadjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unadjusted values\n",
    "#plot WS and SW together\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.ylim(0,1000)\n",
    "plt.xlim(0,1)\n",
    "\n",
    "#WS is blue\n",
    "WST_adj = sns.regplot(x['r2_mean'], x['WS_Total'], marker=\"+\", scatter_kws={'alpha':0.5}, label='WS') \n",
    "\n",
    "#SW is orange\n",
    "sns.regplot(x['r2_mean'], x['SW_Total'], marker=\"+\", scatter_kws={'alpha':0.5}, label='SW') \n",
    "\n",
    "plt.ylabel('Mutation Counts')\n",
    "plt.xlabel('R^2 Mean')\n",
    "plt.legend(loc='upper right', prop={'size': 20}, markerscale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.linregress(x['r2_mean'], x['WS_Total']) #WS is blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.linregress(x['r2_mean'], x['SW_Total']) #SW is orange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z_score, input order in slope1, std_error1, slope2, std_error2\n",
    "Z_score(-333.0772246646324, 14.575893956648743, -596.2502791390713, 21.889007231972794)\n",
    "#Z score: 10.00733728308455\n",
    "#p-value: 1.415e-23, reject null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SW/WS, adjusted\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.ylim(0,6)\n",
    "plt.xlim(0,1)\n",
    "\n",
    "WST_adj = sns.regplot(x['r2_mean'], x['Lambda'], marker=\"+\", scatter_kws={'alpha':0.5}, label='Lambda') \n",
    "scipy.stats.linregress(x['r2_mean'], x['Lambda']) \n",
    "\n",
    "plt.ylabel('Lambda')\n",
    "plt.xlabel('R^2 Mean')\n",
    "plt.legend(loc='upper right', prop={'size': 20}, markerscale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.linregress(x['r2_mean'], x['Lambda']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unadjusted Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unadjusted SW/WS\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.ylim(0,3)\n",
    "plt.xlim(0,1)\n",
    "\n",
    "WST_adj = sns.regplot(x['r2_mean'], x['SW_Total']/x['WS_Total'], marker=\"+\", \n",
    "                      scatter_kws={'alpha':0.5}, label='Lambda, raw') \n",
    "\n",
    "plt.ylabel('Lambda')\n",
    "plt.xlabel('R^2 Mean')\n",
    "plt.legend(loc='upper right', prop={'size': 20}, markerscale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.linregress(x['r2_mean'], x['SW_Total']/x['WS_Total']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S2: The 10%, Rares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot rare occuring mutations both direction (SW and WS)\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.ylim(0,500)\n",
    "plt.xlim(0,1)\n",
    "\n",
    "#WS is blue\n",
    "WST_adj = sns.regplot(x['r2_mean'], x['WS_Rare'], marker=\"+\", scatter_kws={'alpha':0.5}, label='WS') \n",
    "\n",
    "#SW is orange\n",
    "sns.regplot(x['r2_mean'], x['SW_Rare'], marker=\"+\", scatter_kws={'alpha':0.5}, label='SW') \n",
    "\n",
    "plt.ylabel('Rare Mutation Counts')\n",
    "plt.xlabel('R^2 Mean')\n",
    "plt.legend(loc='upper right', prop={'size': 20}, markerscale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.linregress(x['r2_mean'], x['WS_Rare']) #WS is blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.linregress(x['r2_mean'], x['SW_Rare']) #SW is orange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z_score, input order in slope1, std_error1, slope2, std_error2\n",
    "Z_score(-137.42182589971887, 5.97001400336962, -331.3243591219772, 11.574309307219929)\n",
    "#Z score: 14.88892207971511\n",
    "#p-value: 3.89e-50, reject null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjusted (note, order reversed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjusted for GC%\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.ylim(0,1200)\n",
    "plt.xlim(0,1)\n",
    "\n",
    "#WS is blue\n",
    "WST_adj = sns.regplot(x['r2_mean'], x['WS_Rare']/(1 - x['GC_Content']), marker=\"+\", scatter_kws={'alpha':0.5}, label='WS') \n",
    "\n",
    "#SW is orange\n",
    "sns.regplot(x['r2_mean'], x['SW_Rare']/x['GC_Content'], marker=\"+\", scatter_kws={'alpha':0.5}, label='SW') \n",
    "\n",
    "plt.ylabel('GC% Adjusted Rare Mutation Counts')\n",
    "plt.xlabel('R^2 Mean')\n",
    "plt.legend(loc='upper right', prop={'size': 20}, markerscale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.linregress(x['r2_mean'], x['WS_Rare']/(1 - x['GC_Content'])) #WS is blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.linregress(x['r2_mean'], x['SW_Rare']/x['GC_Content']) #SW is orange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z_score, input order in slope1, std_error1, slope2, std_error2\n",
    "Z_score(-231.09078524314677, 9.244594424933686, -816.0914951965949, 31.308788830152398)\n",
    "#Z score: 17.920013931749107\n",
    "#p-value: 8.231e-72, two-tailed, reject null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3: The 50%, Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Common, both directions\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.ylim(0,200)\n",
    "plt.xlim(0,1)\n",
    "\n",
    "#WS is blue\n",
    "WST_adj = sns.regplot(x['r2_mean'], x['WS_Common'], marker=\"+\", scatter_kws={'alpha':0.5}, label='WS') \n",
    "\n",
    "#SW is orange\n",
    "sns.regplot(x['r2_mean'], x['SW_Common'], marker=\"+\", scatter_kws={'alpha':0.5}, label='SW') \n",
    "\n",
    "plt.ylabel('Common Mutation Counts')\n",
    "plt.xlabel('R^2 Mean')\n",
    "plt.legend(loc='upper right', prop={'size': 20}, markerscale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.linregress(x['r2_mean'], x['WS_Common']) #WS is blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.linregress(x['r2_mean'], x['SW_Common']) #SW is orange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z_score, input order in slope1, std_error1, slope2, std_error2\n",
    "Z_score(-124.50003857900936, 5.803275326954651, -149.01341361218874, 6.617117051168298)\n",
    "#Z score: 2.7851748838878825\n",
    "#p-value: 0.00535, reject null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjusted for GC content (note reversed order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjusted for GC%\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.ylim(0,600)\n",
    "plt.xlim(0,1)\n",
    "\n",
    "#WS is blue\n",
    "WST_adj = sns.regplot(x['r2_mean'], x['WS_Common']/(1 - x['GC_Content']), marker=\"+\", scatter_kws={'alpha':0.5}, label='WS') \n",
    "\n",
    "#SW is orange\n",
    "sns.regplot(x['r2_mean'], x['SW_Common']/x['GC_Content'], marker=\"+\", scatter_kws={'alpha':0.5}, label='SW') \n",
    "\n",
    "plt.ylabel('GC% Adjusted Common Mutation Counts')\n",
    "plt.xlabel('R^2 Mean')\n",
    "plt.legend(loc='upper right', prop={'size': 20}, markerscale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.linregress(x['r2_mean'], x['WS_Common']/(1 - x['GC_Content'])) #WS is blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.linregress(x['r2_mean'], x['SW_Common']/x['GC_Content']) #SW is orange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z_score, input order in slope1, std_error1, slope2, std_error2\n",
    "Z_score(-206.97054727948316, 8.95909717632094, -373.4737218168506, 19.07827699119179)\n",
    "#Z score: 7.899702175042606\n",
    "#p-value: 2.796e-15, reject null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two window comparison, Odds Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting for datapoints within the window or r2_mean 0.35~0.45 and 0.7~0.9\n",
    "bombus_slice_1 = x[x['r2_mean'].between(0.35, 0.45, inclusive=True)];\n",
    "bombus_slice_2 = x[x['r2_mean'].between(0.7,0.9, inclusive=True)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bombus_slice_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bombus_slice_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorizing data for Odds ratio, no adjustments for GC%, as it would make no difference.\n",
    "#first for first window, denoted slice 1\n",
    "sliced1_WS_R_adj = bombus_slice_1['WS_Rare'].sum();\n",
    "sliced1_WS_C_adj = bombus_slice_1['WS_Common'].sum();\n",
    "\n",
    "sliced1_SW_R_adj = bombus_slice_1['SW_Rare'].sum();\n",
    "sliced1_SW_C_adj = bombus_slice_1['SW_Common'].sum();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OR slice 1, WS/SW, which with Common/Rare is:\n",
    "(sliced1_WS_C_adj/sliced1_WS_R_adj)/(sliced1_SW_C_adj/sliced1_SW_R_adj)\n",
    "#1.8812483529947406"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chi square, slice 1\n",
    "chi_slice1 = np.array ([[sliced1_SW_R_adj, sliced1_SW_C_adj], [sliced1_WS_R_adj, sliced1_WS_C_adj]]) \n",
    "#array setup: SW-Rare,Common and WS-Rare,Common\n",
    "chi2_contingency(chi_slice1)\n",
    "#this returns chi-sqaure, p, degrees of freedom, and expected values in array\n",
    "#(3318.7735798257568, 0.0, 1, array([[63361.04007827, 34481.95992173],[35928.95992173, 19553.04007827]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorizing data for Odds ratio, adjusted for GC%\n",
    "#second window, denoted slice 2\n",
    "sliced2_WS_R_adj = bombus_slice_2['WS_Rare'].sum();\n",
    "sliced2_WS_C_adj = bombus_slice_2['WS_Common'].sum();\n",
    "\n",
    "sliced2_SW_R_adj = bombus_slice_2['SW_Rare'].sum();\n",
    "sliced2_SW_C_adj = bombus_slice_2['SW_Common'].sum();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OR slice 2, WS/SW is:\n",
    "(sliced2_WS_C_adj/sliced2_WS_R_adj)/(sliced2_SW_C_adj/sliced2_SW_R_adj)\n",
    "#1.7314384541973955"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chi square, slice 2\n",
    "chi_slice2 = np.array ([[sliced2_SW_R_adj, sliced2_SW_C_adj], [sliced2_WS_R_adj, sliced2_WS_C_adj]]) \n",
    "#array setup: SW-Rare,Common and WS-Rare,Common\n",
    "chi2_contingency(chi_slice2)\n",
    "#this returns chi-sqaure, p, degrees of freedom, and expected values in array\n",
    "#(1672.9979163350395, 0.0, 1, array([[42443.68350748, 22129.31649252],[25117.31649252, 13095.68350748]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
