{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "import allel\n",
    "import itertools\n",
    "import os\n",
    "from subprocess import call\n",
    "from tqdm import tqdm, trange\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "#allows multiple outputs: all, last, last_expr(default), none, last_expr_or_assign\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"last_expr_or_assign\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: make contig list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First make contig list\n",
    "contig_list = pd.read_table('/data3/arshad_PNAS_data/For_Tae/polistes_contig.txt', sep=',', header=None)\n",
    "contig_list.rename(columns ={0:\"ID\", 1:\"Length\"}, inplace =True)\n",
    "contig_list.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contig_list['ID'] = contig_list['ID'].str.split('ID=').str.get(1)\n",
    "contig_list['Length'] = contig_list['Length'].str.split('=').str.get(1)\n",
    "contig_list['Length'] = contig_list['Length'].str.split('>').str.get(0).astype(int)\n",
    "contig_list.head(3)\n",
    "contig_list.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contig_list['Length'].sort_values(ascending=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contig_plus10k = contig_list[contig_list['Length'] > 100000] #remove anything smaller than single window\n",
    "contig_plus10k['Length'].sort_values();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output the File\n",
    "#Do it once! only once!\n",
    "#contig_plus10k['ID'].to_csv('/home/taeyoon/VcfFiles/LdByWindow/pol_contig_list.txt', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Chop by 100k window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this step uses tabix\n",
    "#this first requires turning vcf to vcf.gz\n",
    "# bgzip your.vcf\n",
    "# tabix -p vcf your.vcf.gz <- this makes index\n",
    "# tabix your.vcf.gz chr1:10,000,000-20,000,000\n",
    "\n",
    "#modify chopbywindow.script.txt (is in VcfFiles/LdByWindows)\n",
    "#this chops them into 100k windows, deposites the output into set directory\n",
    "\n",
    "#for window of LD test in chopped files, going with 60 for safe measure.\n",
    "#No MAF cut as per original and purpose\n",
    "#polistes has to run in geno-r2 (NOT hap-r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Calculating Mean-Median R2 per Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take all files and make one dataframe\n",
    "ld_r2 = []\n",
    "\n",
    "path_folder = '/data3/TaeFile/Pol_HapLd/'\n",
    "\n",
    "for file in tqdm(os.listdir(path_folder), total=len(path_folder)):\n",
    "    window = file.split('_headered')[0]\n",
    "    df = pd.read_csv(f'{path_folder}/{file}', sep='\\t')\n",
    "    mean_r2 = df['R^2'].mean()\n",
    "    median_r2 = df['R^2'].median()\n",
    "    ld_r2.append([window, mean_r2, median_r2])\n",
    "\n",
    "#destination of the final file is in home directory = windowed_LD.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(ld_r2, columns = ['ID', 'r2_mean', 'r2_median'])\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Making the Master Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AA caller, based on frequency.\n",
    "#common, high frequency seen as reference/ancestral\n",
    "#rarer, low frequency seen as alternative/derived\n",
    "def AA_caller(frequency, reference, alternative):\n",
    "    if (frequency > 0.5):\n",
    "        return alternative\n",
    "    elif (frequency < 0.5):\n",
    "        return reference\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mutation direction function \"strength_classifier\"\n",
    "strong_bases= ['G', 'C']\n",
    "weak_bases= ['A', 'T']\n",
    "\n",
    "def strength_classifier(ancestor, derived):\n",
    "    if (ancestor in strong_bases) and (derived in weak_bases):\n",
    "        return 'SW'\n",
    "    elif (ancestor in weak_bases) and (derived in strong_bases):\n",
    "        return 'WS'\n",
    "    else:\n",
    "        return 'NN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define frequency of the Derived State, the mutation\n",
    "#if ALT = Derived, keep the original AF, which describes the frequency of the ALT\n",
    "#if REF = Derived, use 1 - AF\n",
    "\n",
    "def mutation_frequency (Derived, ALT, AF):\n",
    "    if Derived == ALT: #this means derived is ALT, which AF is associated with\n",
    "        return AF\n",
    "    if Derived != ALT: #this means dervied is REF, which is inversely associated with AF\n",
    "        return (1-AF)\n",
    "    else:\n",
    "        return 'Error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split_bar = Trimmed_table(Trimmed_table['Alt_Freq' == 0.1], Trimmed_table['Strength' == 'SW'])\n",
    "#Trimmed_table['Strength'].value_counts().plot(kind='bar')\n",
    "#Ancestry based on allele frequency\n",
    "\n",
    "def barcoder(strength, frequency):\n",
    "    if (strength == 'SW') and (frequency <= 0.1): #make it less or equal, to be generalizable for different data.\n",
    "        return 'SW-Rare'\n",
    "    elif (strength == 'SW') and (0.25 <= frequency <= 0.5):\n",
    "        return 'SW-Common'\n",
    "    elif (strength == 'WS') and (frequency <= 0.1):\n",
    "        return 'WS-Rare'\n",
    "    elif (strength == 'WS') and (0.25 <= frequency <= 0.5):\n",
    "        return 'WS-Common'\n",
    "    else:\n",
    "        return 'NaN'\n",
    "\n",
    "    '''Rare mutation defined as something with frequency of 0.1,\n",
    "        Common mutation is something that is found more than 0.5 of the time.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#execute everything\n",
    "chopped_polistes = []\n",
    "\n",
    "path_folder_2 = '/data3/TaeFile/Pol_HeaderedVcf'\n",
    "\n",
    "for file in tqdm(os.listdir(path_folder_2), total=len(path_folder_2)):\n",
    "    window = file.split('_headered')[0]\n",
    "    \n",
    "    # Process the file\n",
    "    df_basket = pd.read_table(f'{path_folder_2}/{file}', sep ='\\t', header=None, comment='#')\n",
    "    df_basket.rename(columns={\n",
    "        0:\"SCAF\", \n",
    "        1:\"POS\", \n",
    "        2:\"Id\", \n",
    "        3:\"REF\", \n",
    "        4:\"ALT\", \n",
    "        5:\"quality\", \n",
    "        6:\"filter\", \n",
    "        7:\"INFO\", \n",
    "        8:\"header\", \n",
    "        9:\"1\", 10:\"10\", 11:\"11\", 12:\"2b\", 13:\"3\", 14:\"4\", 15:\"5\", 16:\"6\", 17:\"7\", 18:\"8\"}, inplace=True)\n",
    "    \n",
    "    column_picks= [\"SCAF\", \"POS\", \"REF\", \"ALT\", \"INFO\"]\n",
    "    df_basket_picks = df_basket[column_picks]\n",
    "    \n",
    "    # Get Allele Frequency\n",
    "    df_basket_picks['AF'] = df_basket_picks['INFO'].str.split('AF=').str.get(1).str.split(';').str.get(0).astype(float)\n",
    "    df_basket_picks.drop(columns=['INFO'], inplace=True)\n",
    "    \n",
    "    #Drop Allele Frequency of 0 and 1\n",
    "    df_basket_picks = df_basket_picks[df_basket_picks['AF'] != 1.0] #drop all AF of 1\n",
    "    df_basket_picks = df_basket_picks[df_basket_picks['AF'] != 0] #drop all AF of 0\n",
    "    \n",
    "    #AA base calling\n",
    "    df_basket_picks[\"AA\"] = df_basket_picks.apply(lambda row: AA_caller(row[\"AF\"], row[\"REF\"], row[\"ALT\"]), \n",
    "                                                  axis= 'columns')\n",
    "    df_basket_picks[\"Derived\"] = df_basket_picks.apply(lambda row: AA_caller(row[\"AF\"], row[\"ALT\"], row[\"REF\"]), \n",
    "                                                       axis= 'columns')\n",
    "    \n",
    "    #Mutation direction\n",
    "    df_basket_picks['Dirct'] = df_basket_picks.apply(lambda row: strength_classifier(row['REF'], row['ALT']), \n",
    "                                                         axis='columns')\n",
    "    \n",
    "    #Mutation Frequency, feed the variables in order of Derived, ALT, AF\n",
    "    df_basket_picks['MF'] = df_basket_picks.apply(lambda row: mutation_frequency(row['Derived'], row['ALT'], row['AF']), \n",
    "                                                  axis='columns')\n",
    "    \n",
    "    df_basket_picks['Barcode'] = df_basket_picks.apply(lambda row: barcoder(row['Dirct'], row['MF']), axis='columns')\n",
    "    #Barcoded_Wasp = df_basket_picks[df_basket_picks['Barcode'] != 'NaN'] #drop anything NaN <- this tosses NN\n",
    "    #this also got rid of any WS and SW that fell in 0.2 and 0.4 freq window. Now it keeps it all. = better\n",
    "    Barcoded_Wasp = df_basket_picks #maintain variable transition so that I don't have to touch anything downstream\n",
    "    \n",
    "    # dr.kent's stats\n",
    "    SW_Total_freq = (Barcoded_Wasp['Dirct'].values == 'SW').sum()\n",
    "    WS_Total_freq = (Barcoded_Wasp['Dirct'].values == 'WS').sum()\n",
    "    NN_Total_freq = (Barcoded_Wasp['Dirct'].values == 'NN').sum()\n",
    "    SNP_Total = SW_Total_freq + WS_Total_freq + NN_Total_freq\n",
    "    \n",
    "    SW_Rare_freq = (Barcoded_Wasp['Barcode'].values == 'SW-Rare').sum()\n",
    "    WS_Rare_freq = (Barcoded_Wasp['Barcode'].values == 'WS-Rare').sum()\n",
    "    \n",
    "    SW_Common_freq = (Barcoded_Wasp['Barcode'].values == 'SW-Common').sum()\n",
    "    WS_Common_freq = (Barcoded_Wasp['Barcode'].values == 'WS-Common').sum()\n",
    "    \n",
    "    chopped_polistes.append([window, SW_Total_freq, WS_Total_freq, NN_Total_freq, SNP_Total, SW_Rare_freq, WS_Rare_freq, \n",
    "                            SW_Common_freq, WS_Common_freq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_file = pd.DataFrame(chopped_polistes)\n",
    "final_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_file.rename(columns={\n",
    "        0:\"ID\", \n",
    "        1:\"SW_Total\", 2:\"WS_Total\", 3:\"NN_Total\", 4: \"SNP_Total\",\n",
    "        5:\"SW_Rare\", 6:\"WS_Rare\", 7:\"SW_Common\", 8:\"WS_Common\",}, inplace=True)\n",
    "\n",
    "#W_Total_freq, WS_Total_freq, NN_Total_freq, SNP_Total, SW_Rare_freq, WS_Rare_freq, SW_Common_freq, WS_Common_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_file.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start Merging R2 dataframe with the 'final file'\n",
    "merged_Polistes = df.merge(final_file, on='ID')\n",
    "Chopped_Polistes = merged_Polistes.dropna()\n",
    "Chopped_Polistes.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Chopped_Polistes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = final_file[final_file['ID'] == 'PdomSCFr1.2-0173_100000'] #checking if it paired up correctly\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run only once!\n",
    "Chopped_Polistes.to_csv('/home/taeyoon/VcfFiles/LdByWindow/PolistesSFiles/Chopped_Polistes.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Polistes_df = pd.read_csv('/home/taeyoon/VcfFiles/LdByWindow/PolistesSFiles/Chopped_Polistes.csv')\n",
    "Polistes_df.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge with GC content per Window and Adjust Total Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Polistes_GC = pd.read_csv('/home/taeyoon/GCContent/PdomGC_ready.csv', sep='\\t')\n",
    "Polistes_GC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge!\n",
    "Polistes_GC_df = pd.merge(Polistes_df, Polistes_GC, how='left', on=['ID'])\n",
    "Polistes_GC_df.head()\n",
    "len(Polistes_GC_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjusted Total values\n",
    "Polistes_GC_df['SW_T_Adjusted'] = Polistes_GC_df['SW_Total']/Polistes_GC_df['GC_Content']\n",
    "Polistes_GC_df['WS_T_Adjusted'] = Polistes_GC_df['WS_Total']/(1 - Polistes_GC_df['GC_Content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lambda, which is SW/WS\n",
    "Polistes_GC_df['Lambda'] = Polistes_GC_df['SW_T_Adjusted'] / Polistes_GC_df['WS_T_Adjusted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Polistes_GC_df.head()\n",
    "len(Polistes_GC_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concise, easier viewing\n",
    "Polistes_view = Polistes_GC_df.drop(columns=[\n",
    "    'SW_Total','WS_Total','NN_Total','SNP_Total','SW_Rare','WS_Rare','SW_Common','WS_Common'])\n",
    "Polistes_view.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean of lambda\n",
    "Polistes_view['Lambda'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GC content unadjusted\n",
    "(Polistes_GC_df['SW_Total']/Polistes_GC_df['WS_Total']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S1: Looking at Total numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.ncbi.nlm.nih.gov/genome/?term=polistes+dominula%5Borgn%5D\n",
    "#Polistes dominula (european paper wasp)\n",
    "#GC% 31.5% (AT will be 68.5%), compare to B.imp 37.9%, it's a little lower.\n",
    "#this is global value, instead used GC% per 10k window value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name the the final table to work with x, drop possible NaNs\n",
    "x = Polistes_GC_df.dropna()\n",
    "len(x) #pre-dropna is 2096, post treatment is 2095, one Nan dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot WS and SW together\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.ylim(0,3000)\n",
    "plt.xlim(0,1)\n",
    "\n",
    "#WS is blue\n",
    "WST_adj = sns.regplot(x['r2_mean'], x['WS_T_Adjusted'], marker=\"+\", scatter_kws={'alpha':0.5}, label='WS') \n",
    "\n",
    "#SW is orange\n",
    "sns.regplot(x['r2_mean'], x['SW_T_Adjusted'], marker=\"+\", scatter_kws={'alpha':0.5}, label='SW') \n",
    "\n",
    "plt.ylabel('GC% adjusted Mutation Counts')\n",
    "plt.xlabel('R^2 Mean')\n",
    "plt.legend(loc='upper right', prop={'size': 20}, markerscale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.linregress(x['r2_mean'], x['WS_T_Adjusted']) #WS is blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.linregress(x['r2_mean'], x['SW_T_Adjusted']) #SW is orange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z-test for coefficients (slopes)\n",
    "def Z_score(slope1, std_error1, slope2, std_error2):\n",
    "    numerator = (slope1 - slope2)\n",
    "    denominator = pow((pow(std_error1,2) + pow(std_error2,2)), 1/2)\n",
    "    Z = (numerator) / (denominator)\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z_score for SW and WS, adjusted numbers\n",
    "Z_score(-487.4643627632817, 50.747555732926905, -1691.774761691781, 194.24362089937904)\n",
    "#result is 5.998658410170709\n",
    "#two tailed p-value is 1.99e-9, reject null, observed difference is valid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unadjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unadjusted values\n",
    "#plot WS and SW together\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.ylim(0,1000)\n",
    "plt.xlim(0,1)\n",
    "\n",
    "#WS is blue\n",
    "WST_adj = sns.regplot(x['r2_mean'], x['WS_Total'], marker=\"+\", scatter_kws={'alpha':0.5}, label='WS') \n",
    "\n",
    "#SW is orange\n",
    "sns.regplot(x['r2_mean'], x['SW_Total'], marker=\"+\", scatter_kws={'alpha':0.5}, label='SW') \n",
    "\n",
    "plt.ylabel('Mutation Counts')\n",
    "plt.xlabel('R^2 Mean')\n",
    "plt.legend(loc='upper right', prop={'size': 20}, markerscale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.linregress(x['r2_mean'], x['WS_Total']) #WS is blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.linregress(x['r2_mean'], x['SW_Total']) #SW is orange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z_score, input order in slope1, std_error1, slope2, std_error2\n",
    "Z_score(-304.3494499417676, 36.39604121169277, -571.1745603683817, 48.10463877925413)\n",
    "#score 4.423356026798727\n",
    "#p-value: 0.000009718, reject null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda, Odds Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SW/WS, adjusted\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.ylim(0,8)\n",
    "plt.xlim(0,1)\n",
    "\n",
    "WST_adj = sns.regplot(x['r2_mean'], x['Lambda'], marker=\"+\", scatter_kws={'alpha':0.5}, label='Lambda') \n",
    "\n",
    "plt.ylabel('Lambda')\n",
    "plt.xlabel('R^2 Mean')\n",
    "plt.legend(loc='upper right', prop={'size': 20}, markerscale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.linregress(x['r2_mean'], x['Lambda']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unadjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unadjusted SW/WS\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.ylim(0,3)\n",
    "plt.xlim(0,1)\n",
    "\n",
    "WST_adj = sns.regplot(x['r2_mean'], x['SW_Total']/x['WS_Total'], marker=\"+\", scatter_kws={'alpha':0.5}, label='Lambda, raw') \n",
    "\n",
    "plt.ylabel('Lambda')\n",
    "plt.xlabel('R^2 Mean')\n",
    "plt.legend(loc='upper right', prop={'size': 20}, markerscale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.linregress(x['r2_mean'], x['SW_Total']/x['WS_Total']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S2: The 10%, Rares (Note, unadjusted first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot rare occuring mutations both direction (SW and WS)\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.ylim(0,550)\n",
    "plt.xlim(0,1)\n",
    "\n",
    "#WS is blue\n",
    "WST_adj = sns.regplot(x['r2_mean'], x['WS_Rare'], marker=\"+\", scatter_kws={'alpha':0.5}, label='WS') \n",
    "\n",
    "#SW is orange\n",
    "sns.regplot(x['r2_mean'], x['SW_Rare'], marker=\"+\", scatter_kws={'alpha':0.5}, label='SW') \n",
    "\n",
    "plt.ylabel('Rare Mutation Counts')\n",
    "plt.xlabel('R^2 Mean')\n",
    "plt.legend(loc='upper right', prop={'size': 20}, markerscale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.linregress(x['r2_mean'], x['WS_Rare']) #WS is blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.linregress(x['r2_mean'], x['SW_Rare']) #SW is orange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z_score, input order in slope1, std_error1, slope2, std_error2\n",
    "Z_score(-156.3121713769304, 17.77337030619035, -369.5995672084842, 27.379952324000346)\n",
    "#Z = 6.533973394189548\n",
    "#p-value = 6.405e-11, reject null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjusted (Note, the order swtiches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjusted for GC%\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.ylim(0,1800)\n",
    "plt.xlim(0,1)\n",
    "\n",
    "#WS is blue\n",
    "WST_adj = sns.regplot(x['r2_mean'], x['WS_Rare']/(1 - x['GC_Content']), marker=\"+\", scatter_kws={'alpha':0.5}, label='WS') \n",
    "\n",
    "#SW is orange\n",
    "sns.regplot(x['r2_mean'], x['SW_Rare']/x['GC_Content'], marker=\"+\", scatter_kws={'alpha':0.5}, label='SW') \n",
    "\n",
    "plt.ylabel('GC% Adjusted Rare Mutation Counts')\n",
    "plt.xlabel('R^2 Mean')\n",
    "plt.legend(loc='upper right', prop={'size': 20}, markerscale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.linregress(x['r2_mean'], x['WS_Rare']/(1 - x['GC_Content'])) #WS is blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.linregress(x['r2_mean'], x['SW_Rare']/x['GC_Content']) #SW is orange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z_score, input order in slope1, std_error1, slope2, std_error2\n",
    "Z_score(-238.08174907285255, 24.06764227985072, -1084.6426887599052, 105.00682585428217)\n",
    "#Z = 7.858195678676494\n",
    "#p-value = 3.897e-15, reject null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3: The 50%, Common (Note, unadjusted first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Common, both directions\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.ylim(0,200)\n",
    "plt.xlim(0,1)\n",
    "\n",
    "#WS is blue\n",
    "WST_adj = sns.regplot(x['r2_mean'], x['WS_Common'], marker=\"+\", scatter_kws={'alpha':0.5}, label='WS') \n",
    "\n",
    "#SW is orange\n",
    "sns.regplot(x['r2_mean'], x['SW_Common'], marker=\"+\", scatter_kws={'alpha':0.5}, label='SW') \n",
    "\n",
    "plt.ylabel('Common Mutation Counts')\n",
    "plt.xlabel('R^2 Mean')\n",
    "plt.legend(loc='upper right', prop={'size': 20}, markerscale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.linregress(x['r2_mean'], x['WS_Common']) #WS is blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.linregress(x['r2_mean'], x['SW_Common']) #SW is orange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z_score, input order in slope1, std_error1, slope2, std_error2\n",
    "Z_score(-89.42915500513821, 12.095709120036611, -104.4788070065201, 13.089381012355068)\n",
    "#Z = 0.8444232459755118\n",
    "#p-value = 0.3984, fail to reject null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjusted for GC content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjusted for GC%\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.ylim(0,650)\n",
    "plt.xlim(0,1)\n",
    "\n",
    "#WS is blue\n",
    "WST_adj = sns.regplot(x['r2_mean'], x['WS_Common']/(1 - x['GC_Content']), marker=\"+\", scatter_kws={'alpha':0.5}, label='WS') \n",
    "\n",
    "#SW is orange\n",
    "sns.regplot(x['r2_mean'], x['SW_Common']/x['GC_Content'], marker=\"+\", scatter_kws={'alpha':0.5}, label='SW') \n",
    "\n",
    "plt.ylabel('GC% Adjusted Common Mutation Counts')\n",
    "plt.xlabel('R^2 Mean')\n",
    "plt.legend(loc='upper right', prop={'size': 20}, markerscale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.linregress(x['r2_mean'], x['WS_Common']/(1 - x['GC_Content'])) #WS is blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.linregress(x['r2_mean'], x['SW_Common']/x['GC_Content']) #SW is orange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z_score, input order in slope1, std_error1, slope2, std_error2\n",
    "Z_score(-134.4124150583511, 16.37546777805637, -308.6510263900377, 52.4629274192974)\n",
    "#Z = 3.1703260148763035\n",
    "#p-value = 0.001523, reject null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two window comparison, Odds Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting for datapoints within the window or r2_mean 0.2-0.3 and 0.4-0.6\n",
    "polistes_slice_1 = x[x['r2_mean'].between(0.2, 0.3, inclusive=True)];\n",
    "polistes_slice_2 = x[x['r2_mean'].between(0.4,0.6, inclusive=True)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polistes_slice_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polistes_slice_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorizing data for Odds ratio, adjusted for GC%\n",
    "#first for first window, denoted slice 1\n",
    "sliced1_WS_R_adj = polistes_slice_1['WS_Rare'].sum();\n",
    "sliced1_WS_C_adj = polistes_slice_1['WS_Common'].sum();\n",
    "\n",
    "sliced1_SW_R_adj = polistes_slice_1['SW_Rare'].sum();\n",
    "sliced1_SW_C_adj = polistes_slice_1['SW_Common'].sum();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OR slice 1, WS/SW is:\n",
    "(sliced1_WS_C_adj/sliced1_WS_R_adj)/(sliced1_SW_C_adj/sliced1_SW_R_adj)\n",
    "#1.6270959290370026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chi square, slice 1\n",
    "chi_slice1 = np.array ([[sliced1_SW_R_adj, sliced1_SW_C_adj], [sliced1_WS_R_adj, sliced1_WS_C_adj]]) \n",
    "#array setup: SW-Rare,Common and WS-Rare,Common\n",
    "chi2_contingency(chi_slice1)\n",
    "#this returns chi-sqaure, p, degrees of freedom, and expected values in array\n",
    "#(12168.411007836867, 0.0, 1, array([[421459.3104242, 185673.6895758], [266726.6895758, 117506.3104242]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorizing data for Odds ratio, adjusted for GC%\n",
    "#second window, denoted slice 2\n",
    "sliced2_WS_R_adj = polistes_slice_2['WS_Rare'].sum();\n",
    "sliced2_WS_C_adj = polistes_slice_2['WS_Common'].sum();\n",
    "\n",
    "sliced2_SW_R_adj = polistes_slice_2['SW_Rare'].sum();\n",
    "sliced2_SW_C_adj = polistes_slice_2['SW_Common'].sum();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OR slice 2, WS/SW is:\n",
    "(sliced2_WS_C_adj/sliced2_WS_R_adj)/(sliced2_SW_C_adj/sliced2_SW_R_adj)\n",
    "#1.5470135292224483"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chi square, slice 2\n",
    "chi_slice2 = np.array ([[sliced2_SW_R_adj, sliced2_SW_C_adj], [sliced2_WS_R_adj, sliced2_WS_C_adj]]) \n",
    "#array setup: SW-Rare,Common and WS-Rare,Common\n",
    "chi2_contingency(chi_slice2)\n",
    "#this returns chi-sqaure, p, degrees of freedom, and expected values in array\n",
    "#(2527.0749701551485, 0.0, 1, array([[102267.93236487,  48062.06763513],[ 67535.06763513,  31738.93236487]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
